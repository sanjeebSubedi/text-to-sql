# ==========================================
# Cell 1: Install Dependencies
# ==========================================
# !pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
# !pip install --no-deps xformers "trl<0.9.0" peft accelerate bitsandbytes

# ==========================================
# Cell 2: Imports
# ==========================================
import os
import json
import shutil
from unsloth import FastLanguageModel, PatchDPOTrainer
import torch
from trl import DPOTrainer, DPOConfig
from datasets import Dataset

# Patch DPO Trainer for Unsloth compatibility
PatchDPOTrainer()

# ==========================================
# Cell 3: Configuration
# ==========================================
MODEL_TYPE = "qwen"  # "qwen" or "phi3"

CONFIG = {
    "qwen": {
        "sft_adapter_path": "/content/drive/MyDrive/text_to_sql/checkpoints/final_adapter_qwen",
        # Point to dpo_samples.json generated by create_dpo_dataset.py
        "dpo_data_path": "/content/drive/MyDrive/text_to_sql/dpo_qwen/dpo_samples.json",
        "output_dir": "spider_qwen_dpo",
        "drive_output": "/content/drive/MyDrive/text_to_sql/checkpoints/final_adapter_qwen_dpo",
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    },
    "phi3": {
        "sft_adapter_path": "/content/drive/MyDrive/text_to_sql/checkpoints/final_adapter_phi3",
        "dpo_data_path": "/content/drive/MyDrive/text_to_sql/dpo_phi3/dpo_samples.json",
        "output_dir": "spider_phi3_dpo",
        "drive_output": "/content/drive/MyDrive/text_to_sql/checkpoints/final_adapter_phi3_dpo",
        "target_modules": ["qkv_proj", "o_proj", "gate_up_proj", "down_proj"],
    }
}

# DPO Hyperparameters
DPO_PARAMS = {
    "beta": 0.1,
    "learning_rate": 5e-6,
    "num_epochs": 1,
    "batch_size": 4,
    "gradient_accumulation_steps": 4,
    "max_length": 2048,
    "max_prompt_length": 1536,
}

cfg = CONFIG[MODEL_TYPE]
print("=" * 60)
print(f"DPO TRAINING: {MODEL_TYPE.upper()}")
print("=" * 60)
print(f"SFT Adapter: {cfg['sft_adapter_path']}")
print(f"DPO Data: {cfg['dpo_data_path']}")
print(f"Beta: {DPO_PARAMS['beta']}, LR: {DPO_PARAMS['learning_rate']}")

# ==========================================
# Cell 4: Load Model
# ==========================================
print("\nLoading SFT model...")

# IMPORTANT: Load the SFT adapter directly - do NOT call get_peft_model()
# The adapter is already attached when loading from a saved adapter path
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=cfg["sft_adapter_path"],
    max_seq_length=DPO_PARAMS["max_length"],
    dtype=None,
    load_in_4bit=True,
)

# Enable gradient checkpointing for memory efficiency during training
model.gradient_checkpointing_enable()

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("SFT Model loaded! (Preserving trained LoRA weights for DPO)")

# ==========================================
# Cell 5: Load DPO Dataset
# ==========================================
print("\nLoading DPO dataset...")

with open(cfg["dpo_data_path"]) as f:
    dpo_data = json.load(f)

print(f"Loaded {len(dpo_data)} DPO pairs")

dataset = Dataset.from_dict({
    "prompt": [d["prompt"] for d in dpo_data],
    "chosen": [d["chosen"] for d in dpo_data],
    "rejected": [d["rejected"] for d in dpo_data],
})

split = dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = split["train"]
eval_dataset = split["test"]

print(f"Train: {len(train_dataset)}, Eval: {len(eval_dataset)}")

# ==========================================
# Cell 6: DPO Trainer
# ==========================================
print("\nSetting up DPO Trainer...")

trainer = DPOTrainer(
    model=model,
    ref_model=None,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    args=DPOConfig(
        output_dir=cfg["output_dir"],
        beta=DPO_PARAMS["beta"],
        per_device_train_batch_size=DPO_PARAMS["batch_size"],
        gradient_accumulation_steps=DPO_PARAMS["gradient_accumulation_steps"],
        num_train_epochs=DPO_PARAMS["num_epochs"],
        learning_rate=DPO_PARAMS["learning_rate"],
        max_length=DPO_PARAMS["max_length"],
        max_prompt_length=DPO_PARAMS["max_prompt_length"],
        optim="adamw_8bit",
        lr_scheduler_type="cosine",
        warmup_ratio=0.1,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        eval_strategy="steps",
        eval_steps=100,
        seed=42,
        report_to="none",
    ),
)

print("Trainer ready!")

# ==========================================
# Cell 7: Train
# ==========================================
print("\n" + "=" * 60)
print("STARTING DPO TRAINING")
print("=" * 60)

trainer_stats = trainer.train()

print(f"\nTraining Complete! Loss: {trainer_stats.training_loss:.4f}")

# ==========================================
# Cell 8: Save Model
# ==========================================
final_path = os.path.join(cfg["output_dir"], "final_adapter")
model.save_pretrained(final_path)
tokenizer.save_pretrained(final_path)
print(f"Saved to: {final_path}")

os.makedirs(os.path.dirname(cfg["drive_output"]), exist_ok=True)
shutil.copytree(final_path, cfg["drive_output"], dirs_exist_ok=True)
print(f"Copied to Drive: {cfg['drive_output']}")

# ==========================================
# Cell 9: Quick Test
# ==========================================
print("\n" + "=" * 60)
print("QUICK TEST")
print("=" * 60)

FastLanguageModel.for_inference(model)

schema = "Table: users\nColumns: id, name, email, age"
question = "How many users are there?"

# Build prompt based on model type
im_s = "<" + "|im_start|" + ">"
im_e = "<" + "|im_end|" + ">"
phi_sys = "<" + "|system|" + ">"
phi_user = "<" + "|user|" + ">"
phi_asst = "<" + "|assistant|" + ">"
phi_end = "<" + "|end|" + ">"

if MODEL_TYPE == "qwen":
    prompt = f"{im_s}system\nYou are a SQL query generator. Output ONLY raw SQL.{im_e}\n"
    prompt += f"{im_s}user\n### Database Schema:\n{schema}\n\n### Question:\n{question}{im_e}\n"
    prompt += f"{im_s}assistant\n"
    end_token = im_e
else:
    prompt = f"{phi_sys}\nYou are a SQL query generator. Output ONLY raw SQL. {phi_end}\n"
    prompt += f"{phi_user}\n### Database Schema:\n{schema}\n\n### Question:\n{question} {phi_end}\n"
    prompt += f"{phi_asst}\n"
    end_token = phi_end

inputs = tokenizer([prompt], return_tensors="pt").to("cuda")
outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True, pad_token_id=tokenizer.eos_token_id)
decoded = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]

# Extract SQL
if MODEL_TYPE == "qwen":
    sql = decoded.split(f"{im_s}assistant")[-1].split(im_e)[0].strip()
else:
    sql = decoded.split(phi_asst)[-1].split(phi_end)[0].strip()

print(f"Generated SQL: {sql}")
